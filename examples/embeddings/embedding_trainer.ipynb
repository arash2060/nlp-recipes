{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use pre-trained embeddings (as we did in the sentence similarity baseline_deep_dive [notebook](../sentence_similarity/baseline_deep_dive.ipynb)), we can train word embeddings using our own dataset. In this notebook, we demonstrate the training process for producing word embeddings using the word2vec, GloVe, and fastText models. We'll utilize the STS Benchmark dataset for this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Data Loading and Preprocessing](#Load-and-Preprocess-Data)\n",
    "* [Word2Vec](#Word2Vec)\n",
    "* [fastText](#fastText)\n",
    "* [GloVe](#GloVe)\n",
    "* [Concluding Remarks](#Concluding-Remarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: boto in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (1.14.42)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.42 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.17.42)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.42->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.42->boto3->smart-open>=1.8.1->gensim) (0.15.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: googledrivedownloader in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install googledrivedownloader\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set the environment path\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "import numpy as np\n",
    "from utils_nlp.dataset.preprocess import (\n",
    "    to_lowercase,\n",
    "    to_spacy_tokens,\n",
    "    rm_spacy_stopwords,\n",
    ")\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from utils_nlp.dataset import stsbenchmark\n",
    "from utils_nlp.common.timer import Timer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set the path for where your repo is located\n",
    "NLP_REPO_PATH = os.path.join('..','..')\n",
    "\n",
    "# Set the path for where your datasets are located\n",
    "BASE_DATA_PATH = os.path.join(NLP_REPO_PATH, \"data\")\n",
    "\n",
    "# Set the path for location to save embeddings\n",
    "SAVE_FILES_PATH = os.path.join(BASE_DATA_PATH, \"trained_word_embeddings\")\n",
    "if not os.path.exists(SAVE_FILES_PATH):\n",
    "    os.makedirs(SAVE_FILES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 401/401 [00:01<00:00, 238KB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data downloaded to ../../data/raw/stsbenchmark\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Produce a pandas dataframe for the training set\n",
    "train_raw = stsbenchmark.load_pandas_df(BASE_DATA_PATH, file_split=\"train\")\n",
    "\n",
    "# Clean the sts dataset\n",
    "sts_train = stsbenchmark.clean_sts(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.00</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.80</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.60</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.25</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                      sentence1  \\\n",
       "0   5.00                         A plane is taking off.   \n",
       "1   3.80                A man is playing a large flute.   \n",
       "2   3.80  A man is spreading shreded cheese on a pizza.   \n",
       "3   2.60                   Three men are playing chess.   \n",
       "4   4.25                    A man is playing the cello.   \n",
       "\n",
       "                                           sentence2  \n",
       "0                        An air plane is taking off.  \n",
       "1                          A man is playing a flute.  \n",
       "2  A man is spreading shredded cheese on an uncoo...  \n",
       "3                         Two men are playing chess.  \n",
       "4                 A man seated is playing the cello.  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5749, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the size of our dataframe\n",
    "sts_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_spacy_stopwords(\n",
    "    df,\n",
    "    sentence_cols=[\"sentence1\", \"sentence2\"],\n",
    "    stop_cols=[\"sentence1_tokens_rm_stopwords\", \"sentence2_tokens_rm_stopwords\"],\n",
    "    custom_stopwords=[],\n",
    "):\n",
    "    \"\"\"\n",
    "    This function tokenizes the sentence pairs using spaCy and remove\n",
    "    stopwords, defaulting to the spaCy en_core_web_sm model\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe with columns sentence_cols to tokenize.\n",
    "        sentence_cols (list, optional): Column names for the raw sentence\n",
    "        pairs.\n",
    "        stop_cols (list, optional): Column names for the tokenized sentences\n",
    "            without stop words.\n",
    "        custom_stopwords (list of str, optional): List of custom stopwords to\n",
    "            register with the spaCy model.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with new columns stop_cols, each containing a\n",
    "            list of tokens for their respective sentences.\n",
    "    \"\"\"\n",
    "    nlp = en_core_web_sm.load()\n",
    "    if len(custom_stopwords) > 0:\n",
    "        for csw in custom_stopwords:\n",
    "            nlp.vocab[csw].is_stop = True\n",
    "    text_df = df[sentence_cols]\n",
    "    nlp_df = text_df.applymap(lambda x: nlp(x))\n",
    "    stop_df = nlp_df.applymap(lambda doc: [token.text for token in doc if not token.is_stop])\n",
    "    stop_df.columns = stop_cols\n",
    "    return pd.concat([df, stop_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase\n",
    "df_low = to_lowercase(sts_train)  \n",
    "# Tokenize text\n",
    "sts_tokenize = to_spacy_tokens(df_low) \n",
    "# Tokenize with removal of stopwords\n",
    "sts_train_stop = rm_spacy_stopwords(sts_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Append together the two sentence columns to get a list of all tokenized sentences.\n",
    "all_sentences =  sts_train_stop[[\"sentence1_tokens_rm_stopwords\", \"sentence2_tokens_rm_stopwords\"]]\n",
    "# Flatten two columns into one list and remove all sentences that are size 0 after tokenization and stop word removal.\n",
    "sentences = [i for i in all_sentences.values.flatten().tolist() if len(i) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11498"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sentence length is 1 tokens\n",
      "Maximum sentence length is 43 tokens\n",
      "Median sentence length is 6.0 tokens\n"
     ]
    }
   ],
   "source": [
    "sentence_lengths = [len(i) for i in sentences]\n",
    "print(\"Minimum sentence length is {} tokens\".format(min(sentence_lengths)))\n",
    "print(\"Maximum sentence length is {} tokens\".format(max(sentence_lengths)))\n",
    "print(\"Median sentence length is {} tokens\".format(np.median(sentence_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['plane', 'taking', '.'],\n",
       " ['air', 'plane', 'taking', '.'],\n",
       " ['man', 'playing', 'large', 'flute', '.'],\n",
       " ['man', 'playing', 'flute', '.'],\n",
       " ['man', 'spreading', 'shreded', 'cheese', 'pizza', '.'],\n",
       " ['man', 'spreading', 'shredded', 'cheese', 'uncooked', 'pizza', '.'],\n",
       " ['men', 'playing', 'chess', '.'],\n",
       " ['men', 'playing', 'chess', '.'],\n",
       " ['man', 'playing', 'cello', '.'],\n",
       " ['man', 'seated', 'playing', 'cello', '.']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec is a predictive model for learning word embeddings from text (see [original research paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)). Word embeddings are learned such that words that share common contexts in the corpus will be close together in the vector space. There are two different model architectures that can be used to produce word2vec embeddings: continuous bag-of-words (CBOW) or continuous skip-gram. The former uses a window of surrounding words (the \"context\") to predict the current word and the latter uses the current word to predict the surrounding context words. See this [tutorial](https://www.guru99.com/word-embedding-word2vec.html#3) on word2vec for more detailed background on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim Word2Vec model has many different parameters (see [here](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)) but the ones that are useful to know about are:  \n",
    "- size: length of the word embedding/vector (defaults to 100)\n",
    "- window: maximum distance between the word being predicted and the current word (defaults to 5)\n",
    "- min_count: ignores all words that have a frequency lower than this value (defaults to 5)\n",
    "- workers: number of worker threads used to train the model (defaults to 3)\n",
    "- sg: training algorithm; 1 for skip-gram and 0 for CBOW (defaults to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Timer to see how long the model takes to train\n",
    "t = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.start()\n",
    "\n",
    "# Train the Word2vec model\n",
    "word2vec_model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=10, sg=0)\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 0.9831\n"
     ]
    }
   ],
   "source": [
    "print(\"Time elapsed: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained we can:\n",
    "\n",
    "1. Query for the word embeddings of a given word. \n",
    "2. Inspect the model vocabulary\n",
    "3. Save the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [ 0.24245308 -0.22766319  0.03921927 -0.00569307  0.11457231 -0.1936935\n",
      "  0.02717012 -0.02499952 -0.08161789 -0.06850132 -0.12185523  0.09056653\n",
      " -0.16452083 -0.05633897  0.13712841  0.17568235 -0.03181924  0.11394875\n",
      " -0.08278044  0.00519803  0.20588045 -0.14170235  0.1036065  -0.18324244\n",
      "  0.15329956  0.07836024  0.01878471 -0.10772411  0.07382355 -0.27275255\n",
      " -0.01688346 -0.06271663 -0.01128439 -0.13694242  0.00925762  0.09867323\n",
      " -0.09384014  0.04981905  0.08084494  0.05437228 -0.02023421 -0.11140446\n",
      " -0.01054097  0.16789724 -0.03027184  0.02410144 -0.10566749  0.21216291\n",
      " -0.13435307 -0.03032943  0.00840679  0.11766987 -0.16479416  0.0108357\n",
      "  0.15942638  0.21357156  0.1415852   0.01696487 -0.06481776 -0.02605086\n",
      " -0.12410068  0.23055707 -0.18627672 -0.08867729  0.08526563  0.11027125\n",
      "  0.1209132   0.05080904  0.02636342  0.12432348 -0.03489598  0.01239383\n",
      "  0.0515947   0.04784227  0.0122715   0.04592336 -0.01565376  0.1566813\n",
      "  0.17707448 -0.03667117  0.0379415  -0.09323     0.10643032 -0.03267409\n",
      "  0.01621038 -0.02537276 -0.14106256 -0.14029525  0.1039362   0.11239669\n",
      " -0.01570044 -0.03274915 -0.16492794  0.16784641 -0.01846867 -0.25476256\n",
      " -0.21313709 -0.1458964  -0.22068736 -0.12930286]\n",
      "\n",
      "First 30 vocabulary words: ['plane', 'taking', '.', 'air', 'man', 'playing', 'large', 'flute', 'spreading', 'cheese', 'pizza', 'men', 'seated', 'fighting', 'smoking', 'piano', 'guitar', 'singing', 'woman', 'person']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by accessing the \"wv\" attribute and passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", word2vec_model.wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(word2vec_model.wv.vocab)[:20])\n",
    "\n",
    "# 3. Save the word embeddings. We can save as binary format (to save space) or ASCII format.\n",
    "word2vec_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"word2vec_model\", binary=True)  # binary format\n",
    "word2vec_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"word2vec_model\", binary=False)  # ASCII format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText is an unsupervised algorithm created by Facebook Research for efficiently learning word embeddings (see [original research paper](https://arxiv.org/pdf/1607.04606.pdf)). fastText is significantly different than word2vec or GloVe in that these two algorithms treat each word as the smallest possible unit to find an embedding for. Conversely, fastText assumes that words are formed by an n-gram of characters (i.e. 2-grams of the word \"language\" would be {la, an, ng, gu, ua, ag, ge}). The embedding for a word is then composed of the sum of these character n-grams. This has advantages when finding word embeddings for rare words and words not present in the dictionary, as these words can still be broken down into character n-grams. Typically, for smaller datasets, fastText performs better than word2vec or GloVe. See this [tutorial](https://fasttext.cc/docs/en/unsupervised-tutorial.html) on fastText for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gensim fastText model has many different parameters (see [here](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText)) but the ones that are useful to know about are:  \n",
    "- size: length of the word embedding/vector (defaults to 100)\n",
    "- window: maximum distance between the word being predicted and the current word (defaults to 5)\n",
    "- min_count: ignores all words that have a frequency lower than this value (defaults to 5)\n",
    "- workers: number of worker threads used to train the model (defaults to 3)\n",
    "- sg: training algorithm- 1 for skip-gram and 0 for CBOW (defaults to 0)\n",
    "- iter: number of epochs (defaults to 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Timer to see how long the model takes to train\n",
    "t = Timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.start()\n",
    "\n",
    "# Train the FastText model\n",
    "fastText_model = FastText(size=100, window=5, min_count=5, sentences=sentences, iter=30)\n",
    "\n",
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 13.8839\n"
     ]
    }
   ],
   "source": [
    "print(\"Time elapsed: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can utilize the same attributes as we saw above for word2vec due to them both originating from the gensim package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [-0.16347437 -0.7088631  -0.03814459  0.6433484   0.40757865 -0.13082083\n",
      " -0.93840206 -0.37719026  0.3627747   0.06398258  0.39715233  0.1359164\n",
      " -0.2876     -0.53517324 -0.53162557  0.06264706 -0.28391424 -0.16937332\n",
      " -0.05114771 -0.6231778   0.59394616  0.09203725  0.29574725 -0.16855736\n",
      "  0.03266582 -0.07413924  0.43552378  0.21756841 -0.13717045  0.6044816\n",
      "  0.08262639  0.06420626 -0.2889471   0.1491795  -0.273529    0.12348502\n",
      "  0.43662795  0.35906202  0.07150669 -0.03047936  0.54101485  0.3310147\n",
      "  0.20998567  0.18642022  0.07052571  0.15417226 -0.86126775  0.21909595\n",
      " -0.33046338  0.905908   -0.68186295 -0.09857658 -0.25510272  0.41998228\n",
      "  0.4686534  -0.38099957 -0.14052735  0.07257152  0.07026441  0.8441788\n",
      " -0.06001268 -0.14090243 -0.32976252  0.4902379  -0.34447205  0.06513128\n",
      "  0.49258104  0.29840803  0.4146131  -0.0017421  -0.0774308  -0.14005211\n",
      "  0.48880437 -0.5291176  -0.40951633  0.21010408 -0.25720742 -0.07856242\n",
      "  0.39267027 -0.14111987 -0.29735357  0.35227317  0.28605705  0.07908832\n",
      " -0.27011067  0.8109291   0.01414576 -0.4756266   0.13606936  0.08981264\n",
      "  0.22746734  0.13807122  0.29849872  0.30766425  0.01824222 -0.29065767\n",
      " -0.3804196   0.09652477 -0.290003   -0.0809316 ]\n",
      "\n",
      "First 30 vocabulary words: ['plane', 'taking', '.', 'air', 'man', 'playing', 'large', 'flute', 'spreading', 'cheese', 'pizza', 'men', 'seated', 'fighting', 'smoking', 'piano', 'guitar', 'singing', 'woman', 'person']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by accessing the \"wv\" attribute and passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", fastText_model.wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(fastText_model.wv.vocab)[:20])\n",
    "\n",
    "# 3. Save the word embeddings. We can save as binary format (to save space) or ASCII format.\n",
    "fastText_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"fastText_model\", binary=True)  # binary format\n",
    "fastText_model.wv.save_word2vec_format(SAVE_FILES_PATH+\"fastText_model\", binary=False)  # ASCII format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised algorithm for obtaining word embeddings created by the Stanford NLP group (see [original research paper](https://nlp.stanford.edu/pubs/glove.pdf)). Training occurs on word-word co-occurrence statistics with the objective of learning word embeddings such that the dot product of two words' embeddings is equal to the words' probability of co-occurrence. See this [tutorial](https://nlp.stanford.edu/projects/glove/) on GloVe for more detailed background on the model. \n",
    "\n",
    "Gensim doesn't have an implementation of the GloVe model and the other python packages that implement GloVe are unstable, so we leveraged the code directly from the Stanford NLP [repo](https://github.com/stanfordnlp/GloVe). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "gcc src/glove.c -o build/glove -lm -pthread -Ofast -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/glove.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ style comments are not allowed in ISO C90 [enabled by default]\n",
      " //  GloVe: Global Vectors for Word Representation\n",
      "\u001b[01;32m\u001b[K ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K(this will be reported only once per input file) [enabled by default]\n",
      "\u001b[01m\u001b[Ksrc/glove.c:57:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " long long num_lines, *lines_per_thread, vocab_size;\n",
      "\u001b[01;32m\u001b[K      ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kinitialize_parameters\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:67:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long a, b;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kglove_thread\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:105:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long a, b ,l1, l2;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:106:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long id = *(long long*)vid;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:106:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long id = *(long long*)vid;\n",
      "\u001b[01;32m\u001b[K                           ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:114:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "     real* W_updates1 = (real*)malloc(vector_size * sizeof(real));\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:122:26:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kuse of C99 long long integer constant [-Wlong-long]\n",
      "         l1 = (cr.word1 - 1LL) * (vector_size + 1); // cr word indices start at 1\n",
      "\u001b[01;32m\u001b[K                          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:123:27:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kuse of C99 long long integer constant [-Wlong-long]\n",
      "         l2 = ((cr.word2 - 1LL) + vocab_size) * (vector_size + 1); // shift by vocab_size to get separate vectors for context words\n",
      "\u001b[01;32m\u001b[K                           ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:141:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "         real W_updates1_sum = 0;\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ksave_params\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:185:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long a, b;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:199:35:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "         for (a = 0; a < 2 * (long long)vocab_size * (vector_size + 1); a++) fwrite(&W[a], sizeof(real), 1,fout);\n",
      "\u001b[01;32m\u001b[K                                   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:209:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "             for (a = 0; a < 2 * (long long)vocab_size * (vector_size + 1); a++) fwrite(&gradsq[a], sizeof(real), 1,fgs);\n",
      "\u001b[01;32m\u001b[K                                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:232:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "         if (write_header) fprintf(fout, \"%lld %d\\n\", vocab_size, vector_size);\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:239:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < (vector_size + 1); b++) fprintf(fout,\" %lf\", W[a * (vector_size + 1) + b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:240:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < (vector_size + 1); b++) fprintf(fout,\" %lf\", W[(vocab_size + a) * (vector_size + 1) + b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:243:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < vector_size; b++) fprintf(fout,\" %lf\", W[a * (vector_size + 1) + b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:245:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < vector_size; b++) fprintf(fout,\" %lf\", W[a * (vector_size + 1) + b] + W[(vocab_size + a) * (vector_size + 1) + b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:249:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < (vector_size + 1); b++) fprintf(fgs,\" %lf\", gradsq[a * (vector_size + 1) + b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:250:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < (vector_size + 1); b++) fprintf(fgs,\" %lf\", gradsq[(vocab_size + a) * (vector_size + 1) + b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:261:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "             int num_rare_words = vocab_size < 100 ? vocab_size : 100;\n",
      "\u001b[01;32m\u001b[K             ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:272:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < (vector_size + 1); b++) fprintf(fout,\" %lf\", unk_vec[b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:273:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < (vector_size + 1); b++) fprintf(fout,\" %lf\", unk_context[b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:276:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < vector_size; b++) fprintf(fout,\" %lf\", unk_vec[b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:278:17:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "                 for (b = 0; b < vector_size; b++) fprintf(fout,\" %lf\", unk_vec[b] + unk_context[b]);\n",
      "\u001b[01;32m\u001b[K                 ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ktrain_glove\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:294:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long a, file_size;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:308:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     fprintf(stderr,\"Read %lld lines.\\n\", num_lines);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:313:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 0) fprintf(stderr,\"vocab size: %lld\\n\", vocab_size);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:314:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "     if (verbose > 0) fprintf(stderr,\"x_max: %lf\\n\", x_max);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:315:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "     if (verbose > 0) fprintf(stderr,\"alpha: %lf\\n\", alpha);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:316:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "     pthread_t *pt = (pthread_t *)malloc(num_threads * sizeof(pthread_t));\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:317:30:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     lines_per_thread = (long long *) malloc(num_threads * sizeof(long long));\n",
      "\u001b[01;32m\u001b[K                              ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:317:71:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     lines_per_thread = (long long *) malloc(num_threads * sizeof(long long));\n",
      "\u001b[01;32m\u001b[K                                                                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:319:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "     time_t rawtime;\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:327:14:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "         long long *thread_ids = (long long*)malloc(sizeof(long long) * num_threads);\n",
      "\u001b[01;32m\u001b[K              ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:327:39:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "         long long *thread_ids = (long long*)malloc(sizeof(long long) * num_threads);\n",
      "\u001b[01;32m\u001b[K                                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:327:64:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "         long long *thread_ids = (long long*)malloc(sizeof(long long) * num_threads);\n",
      "\u001b[01;32m\u001b[K                                                                ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:327:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "         long long *thread_ids = (long long*)malloc(sizeof(long long) * num_threads);\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:337:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[K%lf\u001b[m\u001b[K’ gnu_printf format [-Wformat=]\n",
      "         fprintf(stderr, \"%s, iter: %03d, cost: %lf\\n\", time_buffer,  b+1, total_cost/num_lines);\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:374:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "     int result = 0;\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/glove.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ksave_params\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/glove.c:287:36:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kfgs\u001b[m\u001b[K’ may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
      "         if (save_gradsq > 0) fclose(fgs);\n",
      "\u001b[01;32m\u001b[K                                    ^\u001b[m\u001b[K\n",
      "gcc src/shuffle.c -o build/shuffle -lm -pthread -Ofast -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ style comments are not allowed in ISO C90 [enabled by default]\n",
      " //  Tool to shuffle entries of word-word cooccurrence files\n",
      "\u001b[01;32m\u001b[K ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K(this will be reported only once per input file) [enabled by default]\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:40:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " long long array_size = 2000000; // size of chunks to shuffle individually\n",
      "\u001b[01;32m\u001b[K      ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kshuffle_by_chunks\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:138:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 0) fprintf(stderr,\"array size: %lld\\n\", array_size);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/shuffle.c:217:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     array_size = (long long) (0.95 * (real)memory_limit * 1073741824/(sizeof(CREC)));\n",
      "\u001b[01;32m\u001b[K                        ^\u001b[m\u001b[K\n",
      "gcc src/cooccur.c -o build/cooccur -lm -pthread -Ofast -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ style comments are not allowed in ISO C90 [enabled by default]\n",
      " //  Tool to calculate word-word cooccurrence statistics\n",
      "\u001b[01;32m\u001b[K ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K(this will be reported only once per input file) [enabled by default]\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:54:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long id;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:59:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " long long max_product; // Cutoff for product of word frequency ranks below which cooccurrence counts will be stored in a compressed full array\n",
      "\u001b[01;32m\u001b[K      ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:60:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " long long overflow_length; // Number of cooccurrence records whose product exceeds max_product to store in memory before writing to disk\n",
      "\u001b[01;32m\u001b[K      ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:107:45:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " void hashinsert(HASHREC **ht, char *w, long long id) {\n",
      "\u001b[01;32m\u001b[K                                             ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:167:32:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " int write_chunk(CREC *cr, long long length, FILE *fout) {\n",
      "\u001b[01;32m\u001b[K                                ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kwrite_chunk\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:170:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long a = 0;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:170:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 forbids mixed declarations and code [-Wpedantic]\n",
      "     long long a = 0;\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmerge_files\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:253:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long counter = 0;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:287:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "         if ((counter%100000) == 0) if (verbose > 1) fprintf(stderr,\"\\033[39G%lld lines.\",counter);\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:298:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     fprintf(stderr,\"\\033[0GMerging cooccurrence files: processed %lld lines.\\n\",++counter);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kget_cooccurrence\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:310:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long a, j = 0, k, id, counter = 0, ind = 0, vocab_size, w1, w2, *lookup, *history;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:316:34:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     history = malloc(sizeof(long long) * window_size);\n",
      "\u001b[01;32m\u001b[K                                  ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:324:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr, \"max product: %lld\\n\", max_product);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:325:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr, \"overflow length: %lld\\n\", overflow_length);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:334:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr, \"loaded %lld words.\\nBuilding lookup table...\", vocab_size);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:337:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     lookup = (long long *)calloc( vocab_size + 1, sizeof(long long) );\n",
      "\u001b[01;32m\u001b[K                    ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:337:63:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     lookup = (long long *)calloc( vocab_size + 1, sizeof(long long) );\n",
      "\u001b[01;32m\u001b[K                                                               ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:347:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr, \"table contains %lld elements.\\n\",lookup[a-1]);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:386:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "         if ((counter%100000) == 0) if (verbose > 1) fprintf(stderr,\"\\033[19G%lld\",counter);\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:395:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "             if (verbose > 2) fprintf(stderr, \"Adding cooccur between words %lld and %lld.\\n\", w1, w2);\n",
      "\u001b[01;32m\u001b[K             ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:395:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:418:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr,\"\\033[0GProcessed %lld tokens.\\n\",counter);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:428:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "         if ( (long long) (0.75*log(vocab_size / x)) < j) {\n",
      "\u001b[01;32m\u001b[K                    ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:429:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "             j = (long long) (0.75*log(vocab_size / x));\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kmain\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:513:25:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     max_product = (long long) n;\n",
      "\u001b[01;32m\u001b[K                         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/cooccur.c:514:29:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     overflow_length = (long long) rlimit/6; // 0.85 + 1/6 ~= 1\n",
      "\u001b[01;32m\u001b[K                             ^\u001b[m\u001b[K\n",
      "gcc src/vocab_count.c -o build/vocab_count -lm -pthread -Ofast -march=native -funroll-loops -Wall -Wextra -Wpedantic\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KC++ style comments are not allowed in ISO C90 [enabled by default]\n",
      " //  Tool to extract unigram counts\n",
      "\u001b[01;32m\u001b[K ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:1:1:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K(this will be reported only once per input file) [enabled by default]\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:39:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long count;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:44:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long count;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:49:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " long long min_count = 1; // min occurrences for inclusion in vocab\n",
      "\u001b[01;32m\u001b[K      ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:50:6:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      " long long max_vocab = 0; // max_vocab = 0 for no limit\n",
      "\u001b[01;32m\u001b[K      ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KCompareVocabTie\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:62:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long c;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KCompareVocab\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:70:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long c;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kget_counts\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:168:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support ‘\u001b[01m\u001b[Klong long\u001b[m\u001b[K’ [-Wlong-long]\n",
      "     long long i = 0, j = 0, vocab_size = 12500;\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:177:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr, \"Processed %lld tokens.\", i);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:188:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "         if (((++i)%100000) == 0) if (verbose > 1) fprintf(stderr,\"\\033[11G%lld tokens.\", i);\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:190:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr, \"\\033[0GProcessed %lld tokens.\\n\", i);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:205:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (verbose > 1) fprintf(stderr, \"Counted %lld unique words.\\n\", j);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:215:13:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "             if (verbose > 0) fprintf(stderr, \"Truncating vocabulary at min count %lld.\\n\",min_count);\n",
      "\u001b[01;32m\u001b[K             ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:218:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "         printf(\"%s %lld\\n\",vocab[i].word,vocab[i].count);\n",
      "\u001b[01;32m\u001b[K         ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:221:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     if (i == max_vocab && max_vocab < j) if (verbose > 0) fprintf(stderr, \"Truncating vocabulary at size %lld.\\n\", max_vocab);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Ksrc/vocab_count.c:222:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[KISO C90 does not support the ‘\u001b[01m\u001b[Kll\u001b[m\u001b[K’ gnu_printf length modifier [-Wformat=]\n",
      "     fprintf(stderr, \"Using vocabulary of size %lld.\\n\\n\", i);\n",
      "\u001b[01;32m\u001b[K     ^\u001b[m\u001b[K\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "glove_model_path = os.path.join(NLP_REPO_PATH, \"utils_nlp\", \"models\", \"glove\")\n",
    "# Execute shell commands\n",
    "!cd $glove_model_path && make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train GloVe vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training GloVe embeddings requires some data prep and then 4 steps (also documented in the original Stanford NLP repo [here](https://github.com/stanfordnlp/GloVe/tree/master/src))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: Prepare Data**\n",
    "   \n",
    "In order to train our GloVe vectors, we first need to save our corpus as a text file with all words separated by 1+ spaces or tabs. Each document/sentence is separated by a new line character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our corpus as tokens delimited by spaces with new line characters in between sentences.\n",
    "training_corpus_file_path = os.path.join(SAVE_FILES_PATH, \"training-corpus-cleaned.txt\")\n",
    "with open(training_corpus_file_path, 'w', encoding='utf8') as file:\n",
    "    for sent in sentences:\n",
    "        file.write(\" \".join(sent) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Timer to see how long the model takes to train\n",
    "t = Timer()\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Build Vocabulary**\n",
    "\n",
    "Run the vocab_count executable. There are 3 optional parameters:\n",
    "1. min-count: lower limit on how many times a word must appear in dataset. Otherwise the word is discarded from our vocabulary.\n",
    "2. max-vocab: upper bound on the number of vocabulary words to keep\n",
    "3. verbose: 0, 1, or 2 (default)\n",
    "\n",
    "Then provide the path to the text file we created in Step 0 followed by a file path that we'll save the vocabulary to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[0GProcessed 85334 tokens.\n",
      "Counted 11716 unique words.\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 2943.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "vocab_count_exe_path = os.path.join(glove_model_path, \"build\", \"vocab_count\")\n",
    "vocab_file_path = os.path.join(SAVE_FILES_PATH, \"vocab.txt\")\n",
    "# Execute shell commands\n",
    "!$vocab_count_exe_path -min-count 5 -verbose 2 <$training_corpus_file_path> $vocab_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Construct Word Co-occurrence Statistics**\n",
    "\n",
    "Run the cooccur executable. There are many optional parameters, but we list the top ones here:\n",
    "1. symmetric: 0 for only looking at left context, 1 (default) for looking at both left and right context\n",
    "2. window-size: number of context words to use (default 15)\n",
    "3. verbose: 0, 1, or 2 (default)\n",
    "4. vocab-file: path/name of the vocabulary file created in Step 1\n",
    "5. memory: soft limit for memory consumption, default 4\n",
    "6. max-product: limit the size of dense co-occurrence array by specifying the max product (integer) of the frequency counts of the two co-occurring words\n",
    "\n",
    "Then provide the path to the text file we created in Step 0 followed by a file path that we'll save the co-occurrences to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 15\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"../../data/trained_word_embeddings/vocab.txt\"...loaded 2943 words.\n",
      "Building lookup table...table contains 8661250 elements.\n",
      "Processing token: 0\u001b[0GProcessed 85334 tokens.\n",
      "Writing cooccurrences to disk......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G0 lines.\u001b[39G100000 lines.\u001b[0GMerging cooccurrence files: processed 188154 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "cooccur_exe_path = os.path.join(glove_model_path, \"build\", \"cooccur\")\n",
    "cooccurrence_file_path = os.path.join(SAVE_FILES_PATH, \"cooccurrence.bin\")\n",
    "# Execute shell commands\n",
    "!$cooccur_exe_path -memory 4 -vocab-file $vocab_file_path -verbose 2 -window-size 15 <$training_corpus_file_path> $cooccurrence_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Shuffle the Co-occurrences**\n",
    "\n",
    "Run the shuffle executable. The parameters are as follows:\n",
    "1. verbose: 0, 1, or 2 (default)\n",
    "2. memory: soft limit for memory consumption, default 4\n",
    "3. array-size: limit to the length of the buffer which stores chunks of data to shuffle before writing to disk\n",
    "\n",
    "Then provide the path to the co-occurrence file we created in Step 2 followed by a file path that we'll save the shuffled co-occurrences to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 188154 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G188154 lines.\u001b[0GMerging temp files: processed 188154 lines.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "shuffle_exe_path = os.path.join(glove_model_path, \"build\", \"shuffle\")\n",
    "cooccurrence_shuf_file_path = os.path.join(SAVE_FILES_PATH, \"cooccurrence.shuf.bin\")\n",
    "# Execute shell commands\n",
    "!$shuffle_exe_path -memory 4 -verbose 2 <$cooccurrence_file_path> $cooccurrence_shuf_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Train GloVe model**\n",
    "\n",
    "Run the glove executable. There are many parameter options, but the top ones are listed below:\n",
    "1. verbose: 0, 1, or 2 (default)\n",
    "2. vector-size: dimension of word embeddings (50 is default)\n",
    "3. threads: number threads, default 8\n",
    "4. iter: number of iterations, default 25\n",
    "5. eta: learning rate, default 0.05\n",
    "6. binary: whether to save binary format (0: text = default, 1: binary, 2: both)\n",
    "7. x-max: cutoff for weighting function, default is 100\n",
    "8. vocab-file: file containing vocabulary as produced in Step 1\n",
    "9. save-file: filename to save vectors to \n",
    "10. input-file: filename with co-occurrences as returned from Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 188154 lines.\n",
      "Initializing parameters...done.\n",
      "vector size: 50\n",
      "vocab size: 2943\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "08/26/20 - 02:40.35AM, iter: 001, cost: 0.078540\n",
      "08/26/20 - 02:40.35AM, iter: 002, cost: 0.072284\n",
      "08/26/20 - 02:40.35AM, iter: 003, cost: 0.070181\n",
      "08/26/20 - 02:40.35AM, iter: 004, cost: 0.066770\n",
      "08/26/20 - 02:40.35AM, iter: 005, cost: 0.063463\n",
      "08/26/20 - 02:40.35AM, iter: 006, cost: 0.060689\n",
      "08/26/20 - 02:40.35AM, iter: 007, cost: 0.058074\n",
      "08/26/20 - 02:40.35AM, iter: 008, cost: 0.056036\n",
      "08/26/20 - 02:40.35AM, iter: 009, cost: 0.053891\n",
      "08/26/20 - 02:40.35AM, iter: 010, cost: 0.051754\n",
      "08/26/20 - 02:40.35AM, iter: 011, cost: 0.049567\n",
      "08/26/20 - 02:40.36AM, iter: 012, cost: 0.047368\n",
      "08/26/20 - 02:40.36AM, iter: 013, cost: 0.045185\n",
      "08/26/20 - 02:40.36AM, iter: 014, cost: 0.043070\n",
      "08/26/20 - 02:40.36AM, iter: 015, cost: 0.041049\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "glove_exe_path = os.path.join(glove_model_path, \"build\", \"glove\")\n",
    "glove_vector_file_path = os.path.join(SAVE_FILES_PATH, \"GloVe_vectors\")\n",
    "# Execute shell commands\n",
    "!$glove_exe_path -save-file $glove_vector_file_path -threads 8 -input-file \\\n",
    "$cooccurrence_shuf_file_path -x-max 10 -iter 15 -vector-size 50 -binary 2 \\\n",
    "-vocab-file $vocab_file_path -verbose 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 17.9333\n"
     ]
    }
   ],
   "source": [
    "print(\"Time elapsed: {}\".format(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we did above for the word2vec and fastText models, let's now inspect our word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the saved word vectors.\n",
    "glove_wv = {}\n",
    "glove_vector_txt_file_path = os.path.join(SAVE_FILES_PATH, \"GloVe_vectors.txt\")\n",
    "with open(glove_vector_txt_file_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        split_line = line.split(\" \")\n",
    "        glove_wv[split_line[0]] = [float(i) for i in split_line[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for apple: [-0.044859, 6.8e-05, -0.022426, 0.026261, -0.06952, 0.029158, 0.01426, -0.039875, -0.025899, 0.138781, -0.062181, -0.001295, 0.174383, 0.080319, -0.05355, -0.006372, 0.065814, 0.046111, 0.017242, 0.011185, -0.08876, -0.020932, 0.033258, -0.099094, -0.044156, 0.016317, -0.025087, -0.033505, 0.261548, 0.099473, -0.025311, 0.085649, -0.08621, -0.106542, 0.100169, 0.071185, -0.049384, -0.183906, -0.058833, -0.225029, -0.031055, 0.229341, -0.003073, -0.128326, 0.094158, 0.104789, 0.045005, 0.001743, -0.047274, 0.133491]\n",
      "\n",
      "First 30 vocabulary words: ['.', ',', 'man', '-', '\"', 'woman', \"'\", 'said', 'dog', 'playing', ':', 'white', 'black', '$', 'killed', 'percent', 'new', 'syria', 'people', 'china']\n"
     ]
    }
   ],
   "source": [
    "# 1. Let's see the word embedding for \"apple\" by passing in \"apple\" as the key.\n",
    "print(\"Embedding for apple:\", glove_wv[\"apple\"])\n",
    "\n",
    "# 2. Inspect the model vocabulary by accessing keys of the \"wv.vocab\" attribute. We'll print the first 20 words.\n",
    "print(\"\\nFirst 30 vocabulary words:\", list(glove_wv.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "In this notebook we have shown how to train word2vec, GloVe, and fastText word embeddings on the STS Benchmark dataset. We also inspected how long each model took to train on our dataset: word2vec took 0.39 seconds, GloVe took 8.16 seconds, and fastText took 10.41 seconds.\n",
    "\n",
    "FastText is typically regarded as the best baseline for word embeddings (see [blog](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)) and is a good place to start when generating word embeddings. Now that we generated word embeddings on our dataset, we could also repeat the baseline_deep_dive notebook using these embeddings (versus the pre-trained ones from the internet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
